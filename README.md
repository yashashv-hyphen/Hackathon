# ğŸ”¬ GazeLab â€” Virtual Lab Platform for Motor-Disabled Students

> *Empowering students with motor disabilities to participate in practical science experiments through the power of AI and eye-tracking technology.*

---

## ğŸ† Hackathon Submission â€” Problem Statement 1
**Track:** Multimodal Assistive Technology for Individuals with Special Needs

---

## ğŸ¯ The Problem

Students with motor disabilities â€” conditions like Parkinson's, cerebral palsy, dystonia, or tremors â€” are systematically **excluded from practical lab work** in schools and universities.

While their peers conduct titrations, build circuits, and perform dissections, motor-disabled students sit on the sidelines. Not because they lack the intellect. Not because they lack the curiosity. But because **no tool exists to bridge the gap between their mind and the lab bench.**

The consequences are real:
- ğŸ“‰ Lower grades due to missed practical assessments
- ğŸ“ Reduced career opportunities in STEM fields
- ğŸ˜” Social exclusion and loss of confidence
- ğŸ”¬ A lifetime of "you can't do this" when they absolutely can

---

## ğŸ’¡ Our Solution â€” GazeLab

**GazeLab** is an AI-powered virtual lab platform where motor-disabled students can perform **real science experiments using only their eyes.**

Upload a PDF lab manual â†’ AI generates the virtual experiment â†’ Student performs it with eye gaze â†’ AI guides, assesses, and teaches in real time.

No hands required. No compromise on learning. Full participation. ğŸ¯

---

## âœ¨ Key Features

### ğŸ‘ï¸ Eye Tracker Control
- Custom-built eye tracking system detects where the student is looking
- Student selects equipment, pours solutions, connects circuits â€” all with eye gaze
- No mouse, no keyboard, no physical contact required

### ğŸ“„ PDF Lab Manual â†’ Virtual Experiment
- Teacher uploads any PDF lab manual
- AI reads and understands the experiment
- Virtual lab environment is **automatically generated** with correct equipment
- Works with ANY school's curriculum, ANY subject, ANY country's syllabus

### ğŸ¤– Agentic AI Assessment Loop
- AI continuously monitors every action the student takes
- Compares against correct procedure in real time
- **Wrong but safe step** â†’ gentle encouraging hint
- **Dangerous step** â†’ dramatic real-world consequence warning
  > *"âš ï¸ In a real lab, mixing these chemicals at this stage would cause a violent exothermic reaction and release toxic fumes!"*
- Teaches real lab discipline, not just button clicking

### â™¿ Disability-Adapted Instructions
- Original lab instructions often require precise hand movements
- Our LLM **rewrites every step** for eye-gaze interaction
- Scientific accuracy preserved, physical barriers removed

### ğŸ™ï¸ Voice-Powered Lab Assistant (RAG Chatbot)
- Student speaks a question â€” no typing needed
- Whisper STT converts speech to text
- AI answers using the **actual lab manual as context** (not generic knowledge)
- Knows exactly which step the student is on
- Encouraging, warm, patient â€” like a real lab supervisor

### ğŸ”’ Real Consequences, Safe Environment
- Virtual environment means zero physical risk
- But consequences of wrong actions are **simulated and explained**
- Student learns why safety matters, not just what to do

---

## ğŸ—ï¸ System Architecture

```
PDF Lab Manual Uploaded
         â†“
   NLP PDF Parser
   (extracts raw structure)
         â†“
   Modifier LLM (Groq/Llama)
   (adapts steps for eye gaze,
    adds equipment mappings)
         â†“
   SQLite Database
   (stores experiment, steps,
    precautions)
         â†“
   Virtual Lab Frontend
   (renders SVG equipment icons,
    activates eye tracker)
         â†“
   Student performs experiment
   with eye gaze
         â†“
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
   AGENTIC AI LOOP (real time)
   Observe â†’ Compare â†’ Critique
   â†’ Feedback â†’ Repeat
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
         â†“
   Voice Chatbot available
   anytime for questions
         â†“
   Experiment Complete â†’
   Assessment Report Generated
```

---

## ğŸ› ï¸ Tech Stack

| Layer | Technology |
|-------|-----------|
| **Backend** | FastAPI + Python |
| **Database** | SQLite + SQLAlchemy |
| **LLM** | Groq API (Llama 3.3 70B) |
| **PDF Parsing** | PyPDF |
| **Speech to Text** | OpenAI Whisper |
| **Eye Tracking** | Custom built (MediaPipe) |
| **Frontend** | React + SVG animations |
| **Server** | Uvicorn |

---

## ğŸ¤– Why This Is Genuinely Agentic AI

GazeLab doesn't just use an LLM as a chatbot. The AI has:

- âœ… A **persistent goal** â€” guide student to complete experiment correctly
- âœ… **Memory** â€” tracks every action student has taken
- âœ… **Tool use** â€” calls different functions based on what it observes
- âœ… **Decision making** â€” independently decides when to intervene
- âœ… **Reflection loop** â€” critiques every step against ground truth
- âœ… **Adaptation** â€” responds differently to dangerous vs safe mistakes

This is a **ReAct pattern agent** running continuously throughout the experiment.

---

## ğŸ“¡ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/process` | Upload PDF lab manual, returns virtual experiment data |
| `POST` | `/action` | Submit student eye gaze action, returns AI feedback |
| `POST` | `/chatbot` | Submit voice question, returns AI answer |

---

## ğŸš€ Getting Started

### Prerequisites
```bash
pip install fastapi uvicorn sqlalchemy pypdf groq openai-whisper torch python-dotenv
```

Also install ffmpeg system-wide:
```bash
# Windows
winget install ffmpeg

# Mac
brew install ffmpeg

# Linux
sudo apt install ffmpeg
```

### Environment Setup
Create a `.env` file in the backend folder:
```
GROQ_API_KEY=your_groq_api_key_here
```

Get your free Groq API key at: [console.groq.com](https://console.groq.com)
if you aren't able to access the API, contact Telegram: @YashashvSamtani

### Run the Server
```bash
uvicorn main:app --reload
```

Server runs at `http://localhost:8000` ğŸš€

---

## ğŸ“ Project Structure

```
backend/
â”œâ”€â”€ main.py              â† FastAPI endpoints
â”œâ”€â”€ llm.py               â† LLM client setup
â”œâ”€â”€ prompts.py           â† All LLM prompts
â”œâ”€â”€ pdf_parser.py        â† PDF text extraction
â”œâ”€â”€ modify_instruct.py   â† Adapts steps for disability
â”œâ”€â”€ comparing_llm.py     â† Agentic assessment loop
â”œâ”€â”€ audio_to_text.py     â† Whisper STT
â”œâ”€â”€ database.py          â† SQLAlchemy engine
â”œâ”€â”€ database_models.py   â† DB table definitions
â”œâ”€â”€ models.py            â† Pydantic schemas
â””â”€â”€ .env                 â† API keys (never commit!)

frontend/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ LabScreen/   â† Virtual lab environment
â”‚   â”‚   â”œâ”€â”€ Chatbot/     â† Voice chatbot UI
â”‚   â”‚   â””â”€â”€ Upload/      â† PDF upload screen
â”‚   â””â”€â”€ assets/
â”‚       â””â”€â”€ equipment/   â† SVG lab equipment icons
â””â”€â”€ hackathon/
    â””â”€â”€ chemistry-lab/   â† Eye tracker core
```

---

## ğŸŒ Impact & Scalability

- ğŸ« Works with **any school's existing PDF lab manuals** â€” zero extra work for teachers
- ğŸŒ Supports **any science subject** â€” chemistry, physics, biology
- â™¾ï¸ **Infinitely expandable** â€” new experiments just require uploading a PDF
- ğŸŒ **Regional friendly** â€” no dependency on specific curriculum
- ğŸ“± **Web based** â€” works on any device with a camera

---

## ğŸ¯ Multimodal Coverage

| Modality | Implementation |
|----------|---------------|
| ğŸ‘ï¸ **Vision** | Eye tracker for all interactions |
| ğŸ”Š **Audio** | Voice input via Whisper STT |
| ğŸ“ **Text** | Instructions, feedback, chatbot responses |
| ğŸ¤– **AI** | Agentic loop + RAG chatbot + step adapter |

---

## ğŸ‘¥ The Problem We're Really Solving

> *"1 billion people worldwide live with some form of disability. Motor disabilities affect millions of students globally. Yet practical science education â€” a gateway to STEM careers â€” remains almost entirely inaccessible to them."*

GazeLab doesn't just give motor-disabled students a workaround. It gives them **full, equal participation** in the most hands-on part of science education.

Because every student deserves to see the pH paper change colour. ğŸ”¬

---

*Built with â¤ï¸ for students who were told they couldn't do practical science. They can now.*
